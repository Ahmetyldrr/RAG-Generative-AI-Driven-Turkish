{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPusL1InIOvhc2w9lbqli/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmetyldrr/RAG-Driven-Generative-AI/blob/main/Ch02_RAG_Embedding_Vector_Stores_with_Deep_Lake_and_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Embedding Vector Stores ile Derin Göl (Deep Lake) ve OpenAI Kullanımı\n",
        "\n",
        "Bu bölümde, RAG (Retrieval-Augmented Generation) tabanlı üretken yapay zeka (Generative AI) uygulamalarında karşılaşılan karmaşıklıklar ve bu karmaşıklıkları aşmak için kullanılan teknikler ele alınacaktır. Özellikle, metinlerin özetlenmesi ve anlamlandırılması için kullanılan \"embedding\" vektörlerinin depolanması ve yönetilmesi konusu derinlemesine incelenecektir.\n",
        "\n",
        "### Embedding Vektörleri ve Önemi\n",
        "\n",
        "Embedding vektörleri, yapılandırılmış veya yapılandırılmamış metinleri kompakt, yüksek boyutlu vektörlere dönüştürerek metinlerin anlamsal özünü yakalar. Bu sayede, bilgi erişimi daha hızlı ve verimli hale gelir. Ancak, büyük veri setleri ile çalışırken, belge embedding'lerinin oluşturulması ve depolanması gerekliliği ortaya çıkar ve bu da depolama sorunlarına yol açar.\n",
        "\n",
        "### Neden Anahtar Kelimeler Yerine Embedding Kullanılır?\n",
        "\n",
        "Anahtar kelimeler yerine embedding vektörlerinin kullanılmasının nedeni, embedding'lerin metinlerin daha derin anlamsal anlamlarını yakalayabilmesi ve daha incelikli, bağlam farkında bilgi erişimi sağlamasıdır. Bu, daha iyi ve daha ilgili sonuçlar elde edilmesini sağlar.\n",
        "\n",
        "## Vektör Mağazaları (Vector Stores)\n",
        "\n",
        "Embedding vektörlerinin organize edilmesi ve hızlı bir şekilde erişilebilmesi için vektör mağazaları kullanılır. Bu bölümde, ham verilerden Activeloop Deep Lake vektör mağazasına nasıl gidileceği, OpenAI embedding modellerinin yüklenmesi ve çeşitli platformlar arası paketlerin kurulması ve uygulanması ele alınacaktır.\n",
        "\n",
        "### RAG Pipeline'ın Bileşenlere Ayrılması\n",
        "\n",
        "RAG pipeline'ını bağımsız bileşenlere ayırarak, birden fazla takımın aynı anda proje üzerinde çalışabilmesi sağlanır. Bu sayede, proje daha verimli ve yönetilebilir hale gelir.\n",
        "\n",
        "## Uygulama: Python ile RAG Pipeline Kurulumu\n",
        "\n",
        "Bu bölümde, Python kullanarak sıfırdan üç bileşenli bir RAG pipeline'ı kurulacaktır. Activeloop Deep Lake, OpenAI ve özel geliştirilen fonksiyonlar kullanılarak, RAG tabanlı üretken yapay zeka pipeline'ı için bir şablon oluşturulacaktır.\n",
        "\n",
        "### Karşılaşılan Zorluklar\n",
        "\n",
        "- **Paket ve Bağımlılık Sorunları:** Çapraz platform ortam sorunları ile paket ve bağımlılıkların yönetimi.\n",
        "- **Veri Parçalama (Chunking), Embedding Vektörleri ve Yükleme:** Veri parçalama, embedding vektörlerinin oluşturulması ve vektör mağazalarına yüklenmesi.\n",
        "\n",
        "### Çözüm: GPT-4o Modeli ile Retrieval Queries Kullanımı\n",
        "\n",
        "GPT-4o modelinin girdisini retrieval queries ile zenginleştirerek sağlam çıktıların üretilmesi sağlanacaktır.\n",
        "\n",
        "## Sonuç\n",
        "\n",
        "Bu bölümün sonunda, vektör mağazalarında gömülü belgelerin üretken yapay zeka için nasıl kullanılacağı tam olarak anlaşılmış olacaktır.\n",
        "\n",
        "### İlgili Kaynaklar\n",
        "\n",
        "- [Activeloop Deep Lake](https://www.activeloop.ai/)\n",
        "- [OpenAI](https://www.openai.com/)\n",
        "\n",
        "### Kullanılan Teknikler ve Kodlar\n",
        "\n",
        "- **Embedding Vektörleri:** Metinlerin anlamsal özünü yakalamak için kullanılır. Örnek kod: `sentence-transformers` kütüphanesini kullanarak embedding oluşturma.\n",
        "- **Vektör Mağazaları:** Embedding vektörlerini organize etmek ve hızlı erişim sağlamak için kullanılır. Örnek: Activeloop Deep Lake.\n",
        "- **RAG Pipeline:** Retrieval-Augmented Generation pipeline'ı, bilgi erişimi ve metin üretimi için kullanılır.\n",
        "\n",
        "### Neden Bu Teknikler Kullanılır?\n",
        "\n",
        "- Daha verimli ve anlamlı bilgi erişimi sağlamak.\n",
        "- Büyük veri setlerini daha iyi yönetmek.\n",
        "- Üretken yapay zeka uygulamalarında daha doğru ve ilgili sonuçlar elde etmek.\n",
        "\n",
        "### Kullanılabilecek Projeler\n",
        "\n",
        "- **Doğal Dil İşleme (NLP) Projeleri:** Metin sınıflandırma, duygu analizi, metin özetleme.\n",
        "- **Üretken Yapay Zeka Uygulamaları:** Chatbot'lar, içerik oluşturma araçları.\n",
        "- **Bilgi Erişim Sistemleri:** Arama motorları, belge yönetim sistemleri.\n",
        "\n",
        "## Markdown İfadeleri ile Özet\n",
        "\n",
        "## RAG ve Embedding Vektörleri\n",
        "## Vektör Mağazaları ve Önemi\n",
        "## Uygulama ve Karşılaşılan Zorluklar\n",
        "## Sonuç ve İlgili Kaynaklar"
      ],
      "metadata": {
        "id": "sAhFf_g8wN8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From raw data to embeddings in vector stores\n",
        "\n",
        "## Giriş\n",
        "Bu bölümde, ham verilerden vektör depolarında (vector stores) gömülü temsillere (embeddings) kadar olan süreç açıklanmaktadır. Gömülü temsiller, metin, resim veya ses gibi her türlü veriyi gerçek sayılara dönüştürerek matematiksel temsiller oluşturur.\n",
        "\n",
        "## Gömülü Templerin Oluşturulması (Creating Embeddings)\n",
        "Gömülü temsiller, OpenAI `text-embedding-3-small` gibi bir model kullanılarak oluşturulur. Bu model, ham verileri (kitaplar, makaleler, bloglar, resimler veya şarkılar) temizledikten sonra gömülü temsillere dönüştürür. Activeloop Deep Lake gibi araçlar, metni önceden tanımlanmış parçalara (chunks) ayırarak daha detaylı gömülü temsiller oluşturur. Bu parçaların boyutu, örneğin 1,000 karakter olabilir.\n",
        "\n",
        "## Şeffaflık ve RAG (Transparency and RAG)\n",
        "RAG (Retrieval-Augmented Generation) çerçeveleri, oluşturulan her içerik parçasının kaynak verilere kadar izlenebilir olmasını sağlar. Bu, çıktıların şeffaflığını garanti eder. OpenAI üretken modeli, artırılmış girdiyi dikkate alarak yanıt verir. Gömülü temsiller doğrudan görünür ve metne bağlıdır, parametrik modellerin aksine.\n",
        "\n",
        "## Vektör Deposu (Vector Store)\n",
        "Vektör deposu, yüksek boyutlu verileri (gömülü temsiller gibi) işlemek üzere tasarlanmış özel bir veritabanıdır. Activeloop gibi sunucusuz platformlarda veri kümeleri oluşturulabilir ve API aracılığıyla kodda erişilebilir.\n",
        "\n",
        "## Vektör Deposunun Özellikleri\n",
        "Vektör depoları, güçlü indeksleme yöntemleri (indexing methods) ile donatılmıştır. Bu, RAG modelinin üretim aşamasında en ilgili gömülü temsilleri hızlı bir şekilde bulmasını ve getirmesini sağlar. Kullanıcı girdilerini artırarak modelin yüksek kaliteli çıktı üretme yeteneğini artırır.\n",
        "\n",
        "## RAG İşlem Hattının Oluşturulması (Building a RAG Pipeline)\n",
        "Veri toplama, işleme ve getirme işlemlerinden artırılmış girdi oluşturmaya kadar olan süreç RAG işlem hattını oluşturur.\n",
        "\n",
        "## Kullanılan Teknikler ve Kodlar\n",
        "- **Gömülü Templerin Oluşturulması**: OpenAI `text-embedding-3-small` modeli kullanılır.\n",
        "- **Vektör Deposu**: Activeloop Deep Lake gibi araçlar kullanılır.\n",
        "- **İndeksleme Yöntemleri**: Vektör depolarında güçlü indeksleme yöntemleri kullanılır.\n",
        "\n",
        "## Neden Bu Teknikler Kullanılır?\n",
        "- Gömülü temsiller, verileri matematiksel olarak temsil ederek benzer verilerin getirilmesini sağlar.\n",
        "- RAG çerçeveleri, çıktıların şeffaflığını garanti eder.\n",
        "- Vektör depoları, yüksek boyutlu verileri verimli bir şekilde işler.\n",
        "\n",
        "## Kullanılabilecek Projeler\n",
        "- Doğal dil işleme (NLP) projeleri\n",
        "- Görüntü ve ses işleme projeleri\n",
        "- Öneri sistemleri (recommendation systems)\n",
        "\n",
        "## Kaynaklar\n",
        "- [Activeloop Deep Lake](https://www.activeloop.ai/)\n",
        "- [OpenAI](https://www.openai.com/)\n",
        "\n",
        "## Ek Teknikler\n",
        "- **Cosine Similarity**: Gömülü temsiller arasındaki benzerliği ölçmek için kullanılır.\n",
        "- **Approximate Nearest Neighbors (ANN)**: Vektör depolarında hızlı arama yapmak için kullanılır.\n",
        "\n",
        "Bu teknikler ve araçlar, büyük miktarda veriyi işleyerek yüksek kaliteli çıktı üretmek için kullanılır."
      ],
      "metadata": {
        "id": "WE5XNvF8xJ8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline'ı Düzenleme (Organizing RAG in a Pipeline)\n",
        "\n",
        "RAG (Retrieve, Augment, Generate) pipeline'ı genellikle verileri toplar, temizler, örneğin dokümanları parçalara ayırır (`chunking`), gömme (`embedding`) işlemlerini gerçekleştirir ve bir vektör veri deposunda (`vector store dataset`) saklar. Daha sonra, vektör veri kümesi (`vector dataset`), üretken bir yapay zeka (`generative AI`) modelinin girdi (`input`) süresini artırmak için sorgulanır (`queried`). Ancak, bir vektör deposu (`vector store`) kullanırken bu RAG işlem sırasını tek bir programda çalıştırmamak önerilir.\n",
        "\n",
        "## İşlemleri Ayırma (Separating the Process)\n",
        "\n",
        "En azından işlemleri üç bileşene ayırmak gerekir:\n",
        "1. **Veri Toplama ve Hazırlama (`Data collection and preparation`)**: Verilerin toplanması ve temizlenmesi.\n",
        "2. **Veri Gömme ve Yükleme (`Data embedding and loading into the dataset of a vector store`)**: Verilerin gömme işlemlerinin yapılması ve vektör veri deposuna yüklenmesi.\n",
        "3. **Sorgulama ve Üretken Model (`Querying the vectorized dataset to augment the input of a generative AI model`)**: Vektör veri kümesinin sorgulanması ve üretken modelin girdi süresini artırmak için kullanılması.\n",
        "\n",
        "## Bu Yaklaşımın Nedenleri (Reasons for this Component Approach)\n",
        "\n",
        "1. **Uzmanlaşma (`Specialization`)**: Her takım üyesinin en iyi olduğu alanda çalışmasına olanak tanır.\n",
        "2. **Ölçeklenebilirlik (`Scalability`)**: Farklı bileşenlerin ayrı ayrı ölçeklendirilmesini ve geliştirilmesini sağlar.\n",
        "3. **Paralel Geliştirme (`Parallel development`)**: Her takımın diğerlerinden bağımsız olarak çalışmasına ve geliştirmeler yapmasına olanak tanır.\n",
        "4. **Bakım (`Maintenance`)**: Bileşenlerin bağımsız olarak bakımının yapılmasını sağlar.\n",
        "5. **Güvenlik ve Gizlilik (`Security concerns and privacy`)**: Her takımın ayrı ayrı yetkilendirilmesini ve erişim kontrolünü sağlar.\n",
        "\n",
        "## Kullanılan Teknikler ve Kodlar (Techniques and Codes Used)\n",
        "\n",
        "- `Chunking`: Dokümanların daha küçük parçalara ayrılması.\n",
        "- `Embedding`: Verilerin vektör temsillerinin oluşturulması.\n",
        "- `Vector Store`: Vektörlerin saklandığı veri deposu.\n",
        "\n",
        "Bu teknikler dışında, RAG pipeline'ında kullanılan diğer teknikler arasında ` Named Entity Recognition (NER)`, `Part-of-Speech (POS) Tagging` ve `Dependency Parsing` gibi doğal dil işleme (`Natural Language Processing, NLP`) teknikleri de bulunabilir.\n",
        "\n",
        "## Neden Böyle Yapılır? (Why is it Done this Way?)\n",
        "\n",
        "RAG pipeline'ını ayrı bileşenlere ayırmak, büyük ölçekli projelerde ve üretim ortamlarında (`production environments`) daha iyi bir yönetim ve bakım sağlar. Ayrıca, her bileşenin bağımsız olarak geliştirilmesi ve ölçeklendirilmesi mümkündür.\n",
        "\n",
        "## Kullanım Alanları (Use Cases)\n",
        "\n",
        "RAG pipeline'ı, metin tabanlı üretken modellerin geliştirilmesinde ve kullanılmasında kullanılabilir. Örneğin, sohbet robotları (`chatbots`), metin özetleme (`text summarization`) ve içerik oluşturma (`content generation`) gibi alanlarda kullanılabilir.\n",
        "\n",
        "## Kaynaklar (Resources)\n",
        "\n",
        "- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
        "- [Vector Databases: A Beginner's Guide](https://www.youtube.com/watch?v=OQBKp9gdHO4)\n",
        "- [A Beginner Guide Vektor Database](https://www.youtube.com/watch?v=NEreO2zlXDk)\n",
        "- [Vector Databases: Complete Tutorial](https://www.youtube.com/watch?v=8KrTO9bS91s)\n",
        "- [What is Vektor Database](https://www.youtube.com/watch?v=gl1r1XV0SLw&pp=ygUPdmVrdG9yIGRhdGFiYXNl)\n",
        "- [Top 5 Vektor Database](https://www.youtube.com/watch?v=csTU7-Wg8Hs)\n",
        "- [OpenAi Embedding Vektor Databese](https://www.youtube.com/watch?v=ySus5ZS0b94)\n",
        "- [Types Vektor Database](https://www.youtube.com/watch?v=VfcRxtBKI54)\n",
        "- [Vektor DataBase PGVektor vs Pinecone](https://www.youtube.com/watch?v=mke1V-2__D0)\n",
        "- [İntroduction Vektör Database](https://www.youtube.com/watch?v=f0EcGl9O_Wg&pp=ygUPdmVrdG9yIGRhdGFiYXNl)\n",
        "- [Vektor DataBase Pinecone](https://www.youtube.com/watch?v=56JSsEbMQVA)\n",
        "-[Vektor DataBase ChromeDb](https://www.youtube.com/watch?v=HjvYsUL8NZQ)\n",
        "- [Natural Language Processing (almost) from Scratch](https://arxiv.org/abs/1103.0398)"
      ],
      "metadata": {
        "id": "J6ZGfVqLx5lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG (Retrieval-Augmented Generation) Tabanlı Üretken Yapay Zeka İşlem Hattı (Pipeline)\n",
        "\n",
        "RAG-driven generative AI pipeline, büyük dil modellerinin (LLM) geliştirilmesinde kullanılan bir yaklaşımdır. Bu yaklaşım, birden fazla bileşeni bir araya getirerek, üretken yapay zeka uygulamalarının geliştirilmesini sağlar.\n",
        "\n",
        "### RAG İşlem Hattının Bileşenleri\n",
        "\n",
        "RAG işlem hattı üç ana bileşenden oluşur:\n",
        "\n",
        "1. **Veri Toplama ve Hazırlama (Data Collection and Prep - D1 ve D2)**: Bu bileşen, ham verilerin toplanması ve temizlenmesi işlemlerini içerir. Bu işlemler, verilerin doğru ve tutarlı olmasını sağlar.\n",
        "2. **Veri Embedding ve Depolama (Data Embedding and Storage - D2 ve D3)**: Bu bileşen, toplanan verilerin OpenAI'ın embedding modeli kullanılarak vektörleştirilmesi ve bu vektörlerin bir vektör veritabanında (örneğin, Activeloop Deep Lake) depolanması işlemlerini içerir.\n",
        "3. **Artırılmış Üretim (Augmented Generation - D4, G1-G4 ve E1)**: Bu bileşen, kullanıcı girdileri ve retrieval sorguları temel alınarak içerik üretimini gerçekleştirir. Bu işlem, GPT-4 gibi büyük dil modelleri kullanılarak yapılır.\n",
        "\n",
        "### RAG İşlem Hattının Avantajları\n",
        "\n",
        "RAG işlem hattının avantajları şunlardır:\n",
        "\n",
        "* **Modüler Yapı**: RAG işlem hattı, her biri farklı bir bileşene odaklanan birden fazla takım tarafından geliştirilebilir. Bu, geliştirme sürecini hızlandırır ve ekiplerin birbirlerini beklemesini önler.\n",
        "* **Esneklik**: RAG işlem hattı, farklı veri kaynakları ve farklı dil modelleri ile entegre edilebilir.\n",
        "\n",
        "### Kullanılan Teknikler\n",
        "\n",
        "RAG işlem hattında kullanılan teknikler şunlardır:\n",
        "\n",
        "* **Embedding**: Metin verilerinin vektörleştirilmesi işlemidir. Bu işlem, OpenAI'ın embedding modeli kullanılarak yapılır.\n",
        "* **Vektör Veritabanı**: Vektörleştirilmiş verilerin depolanması için kullanılan bir veritabanıdır. Örnek olarak Activeloop Deep Lake verilebilir.\n",
        "* **GPT-4**: Büyük bir dil modelidir. Bu model, artırılmış üretim bileşeninde kullanılır.\n",
        "\n",
        "### Neden Böyle Yapılır?\n",
        "\n",
        "RAG işlem hattı, büyük dil modellerinin geliştirilmesini hızlandırmak ve kolaylaştırmak için tasarlanmıştır. Bu yaklaşım, birden fazla bileşeni bir araya getirerek, üretken yapay zeka uygulamalarının geliştirilmesini sağlar.\n",
        "\n",
        "### Kullanılabilecek Projeler\n",
        "\n",
        "RAG işlem hattı, aşağıdaki projelerde kullanılabilir:\n",
        "\n",
        "* **Chatbot**: RAG işlem hattı, chatbot uygulamalarında kullanılabilir.\n",
        "* **İçerik Üretimi**: RAG işlem hattı, içerik üretim uygulamalarında kullanılabilir.\n",
        "* **Dil Çeviri**: RAG işlem hattı, dil çeviri uygulamalarında kullanılabilir.\n",
        "\n",
        "### Kaynaklar\n",
        "\n",
        "* [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2108.12409)\n",
        "* [RAG: Retrieval-Augmented Generation](https://www.pinecone.io/learn/rag-retrieval-augmented-generation/)\n",
        "* [Activeloop Deep Lake](https://www.activeloop.ai/deep-lake/)"
      ],
      "metadata": {
        "id": "KUgZ5XVS0Mab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building RAG Pipeline"
      ],
      "metadata": {
        "id": "kSoFyYRL7bib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline Kurulumu  (Installation Packages and Libraries)\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) pipeline'ı oluşturmak için gerekli paket ve kütüphaneleri kurmak gerekmektedir. Bu işlem sırasında, bağımlılık çakışmalarını (dependency conflicts) ve kütüphanelerin fonksiyonlarıyla ilgili sorunları önlemek için paket sürümlerini dondurmak (freeze package versions) önemlidir.\n",
        "\n",
        "### Bağımlılık Çakışmaları ve Sürüm Sorunları (Dependency Conflicts and Version Issues)\n",
        "\n",
        "- **Sürüm Çakışmaları (Version Conflicts):** Kullanılan kütüphanelerin farklı sürümleri arasında çakışmalar olabilir. Örneğin, bir kütüphane belirli bir sürümdeki başka bir kütüphaneye bağımlı olabilir.\n",
        "- **Uygulama Çalıştırma Sorunları (Application Execution Issues):** Bir uygulamanın çalışması için bazı kütüphanelerin güncellenmesi gerekebilir. Örneğin, Ağustos 2024'te Deep Lake kurulumu için Pillow sürüm 10.x.x gereklirken, Google Colab'da bu sürüm 9.x.x idi. Bu nedenle, Deep Lake kurulumundan önce Pillow'un kaldırılıp yeniden kurulumu gerekmiştir.\n",
        "- **Sürüm Donması (Version Freezing):** Sürüm dondurma, uzun süre aynı sürümde kalındığında bazı kütüphanelerin işlevselliğini kaybetmesine (deprecation) veya hataların düzeltilmemesine neden olabilir.\n",
        "\n",
        "### Paket Sürümlerini Dondurma (Freezing Package Versions)\n",
        "\n",
        "Paket sürümlerini dondurmak, bir uygulama için belirli bir süre boyunca kararlılık sağlar, ancak uzun vadede sorunlara yol açabilir. Diğer taraftan, sürümleri çok hızlı güncelleme de diğer kütüphanelerin işlemez hale gelmesine neden olabilir. Bu nedenle, sürekli bir kalite kontrol süreci (continual quality control process) gereklidir.\n",
        "\n",
        "### Kurulum Adımları (Installation Steps)\n",
        "\n",
        "Bu bölümde, RAG pipeline'ı için gerekli ortamı oluşturmak üzere kurulum adımları gerçekleştirilecektir. Paket sürümleri dondurulacaktır.\n",
        "\n",
        "### Kullanılan Teknikler ve Alternatifler (Used Techniques and Alternatives)\n",
        "\n",
        "- **Paket Sürüm Yönetimi (Package Version Management):** `pip freeze` veya `conda env export` gibi komutlar kullanılarak mevcut paket sürümleri dondurulabilir.\n",
        "- **Ortam Yönetimi (Environment Management):** `conda` veya `virtualenv` gibi araçlar kullanılarak izole edilmiş ortamlar oluşturulabilir.\n",
        "\n",
        "### Kod Örnekleri (Code Examples)\n",
        "\n",
        "```bash\n",
        "# pip kullanarak paket sürümlerini dondurma\n",
        "pip freeze > requirements.txt\n",
        "\n",
        "# conda kullanarak ortamı export etme\n",
        "conda env export > environment.yml\n",
        "```\n",
        "\n",
        "### Neden Böyle Yapılıyor? (Why is it Done This Way?)\n",
        "\n",
        "Paket sürümlerini dondurmak, geliştirme ortamında tutarlılık ve kararlılık sağlar. Ancak, güvenlik yamaları ve yeni özelliklerden yararlanmak için düzenli olarak güncelleme yapmak önemlidir.\n",
        "\n",
        "### Kullanım Alanları (Use Cases)\n",
        "\n",
        "- **Makine Öğrenimi Projeleri (Machine Learning Projects):** Özellikle karmaşık bağımlılıklara sahip makine öğrenimi projelerinde paket sürüm yönetimi kritiktir.\n",
        "- **Yazılım Geliştirme Projeleri (Software Development Projects):** Her türlü yazılım geliştirme projesinde, bağımlılık yönetimi için paket sürüm dondurma ve ortam yönetimi önemlidir.\n",
        "\n",
        "### Kaynaklar (Resources)\n",
        "\n",
        "- [Pip Documentation](https://pip.pypa.io/en/stable/)\n",
        "- [Conda Documentation](https://docs.conda.io/en/latest/)\n",
        "- [Virtualenv Documentation](https://virtualenv.pypa.io/en/latest/)\n",
        "\n",
        "Bu teknikler ve araçlar, yazılım projelerinde kararlılık ve tekrar üretilebilirlik (reproducibility) sağlamak için kullanılır."
      ],
      "metadata": {
        "id": "v0DfOrPJ95UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yapılacak İşlemler ve Teknikler\n",
        "Bu bölümde, bir makine öğrenimi projesinde gerekli olan ortamın kurulması ve yapılandırılması işlemleri anlatılmaktadır. Kullanılan teknikler ve kodlar açıklanacaktır.\n",
        "\n",
        "### 1. Gerekli Paketlerin Kurulması (Installing Requirements)\n",
        "Projede kullanılacak kütüphanelerin kurulması işlemidir. Bu işlem için `pip install` komutu kullanılır. Örneğin:\n",
        "```bash\n",
        "!pip install beautifulsoup4==4.12.3\n",
        "!pip install requests==2.31.0\n",
        "```\n",
        "Bu komutlar, `beautifulsoup4` ve `requests` kütüphanelerini belirtilen sürümlerde kurar.\n",
        "\n",
        "### 2. Google Drive Bağlama (Mounting Google Drive)\n",
        "Google Colab'da çalışırken, Google Drive'ı bağlamak için aşağıdaki kod kullanılır:\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "Bu, Google Drive'ın `/content/drive` dizinine bağlanmasını sağlar.\n",
        "\n",
        "### 3. GitHub'dan Dosya İndirme (Downloading Files from GitHub)\n",
        "GitHub'dan dosya indirmek için `subprocess` modülü kullanılır. Örneğin:\n",
        "```python\n",
        "import subprocess\n",
        "url = \"https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py\"\n",
        "output_file = \"grequests.py\"\n",
        "curl_command = [\"curl\", \"-o\", output_file, url]\n",
        "subprocess.run(curl_command, check=True)\n",
        "```\n",
        "Bu kod, belirtilen URL'den `grequests.py` dosyasını indirir.\n",
        "\n",
        "### 4. Gerekli Kütüphanelerin Kurulması (Installing Requirements)\n",
        "Projede kullanılacak kütüphanelerin kurulması işlemidir. Örneğin:\n",
        "```bash\n",
        "!pip install deeplake==3.9.18\n",
        "!pip install openai==1.40.3\n",
        "```\n",
        "Bu komutlar, `deeplake` ve `openai` kütüphanelerini belirtilen sürümlerde kurar.\n",
        "\n",
        "### 5. DNS Sunucusunun Yapılandırılması (Configuring DNS Server)\n",
        "Activeloop Deep Lake kütüphanesinin çalışması için Google'ın Public DNS sunucusunun yapılandırılması gerekir. Aşağıdaki kod bunu sağlar:\n",
        "```python\n",
        "with open('/etc/resolv.conf', 'w') as file:\n",
        "   file.write(\"nameserver 8.8.8.8\")\n",
        "```\n",
        "Bu, `/etc/resolv.conf` dosyasını `nameserver 8.8.8.8` olarak günceller.\n",
        "\n",
        "### 6. Kimlik Doğrulama (Authentication)\n",
        "OpenAI ve Activeloop API'larına erişmek için kimlik doğrulama işlemleri yapılır. Örneğin:\n",
        "```python\n",
        "# OpenAI API Key\n",
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline().strip()\n",
        "f.close()\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Activeloop API Token\n",
        "f = open(\"drive/MyDrive/files/activeloop.txt\", \"r\")\n",
        "API_token=f.readline().strip()\n",
        "f.close()\n",
        "ACTIVELOOP_TOKEN=API_token\n",
        "os.environ['ACTIVELOOP_TOKEN'] =ACTIVELOOP_TOKEN\n",
        "\n",
        "```\n",
        "\n",
        "## Ornek download kodu\n",
        "\n",
        "```python\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "# add a private token after the filename if necessary\n",
        "def download(directory, filename):\n",
        "    # The base URL of the image files in the GitHub repository\n",
        "    base_url = 'https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/'\n",
        "    # Complete URL for the file\n",
        "    file_url = f\"{base_url}{directory}/{filename}\"\n",
        "    # Use curl to download the file, including an Authorization header for the private token\n",
        "    try:\n",
        "        # Prepare the curl command with the Authorization header\n",
        "        #curl_command = f'curl -H \"Authorization: token {private_token}\" -o {filename} {file_url}'\n",
        "        curl_command = f'curl -H -o {filename} {file_url}'\n",
        "        # Execute the curl command\n",
        "        subprocess.run(curl_command, check=True, shell=True)\n",
        "        print(f\"Downloaded '{filename}' successfully.\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Failed to download '{filename}'. Check the URL, your internet connection, and if the token is correct and has appropriate permissions.\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Bu kodlar, OpenAI ve Activeloop API'larına erişmek için gerekli kimlik doğrulama işlemlerini yapar.\n",
        "\n",
        "## Kullanılan Teknikler ve Nedenleri\n",
        "- `pip install`: Python paketlerini kurmak için kullanılır.\n",
        "- `google.colab.drive.mount`: Google Colab'da Google Drive'ı bağlamak için kullanılır.\n",
        "- `subprocess`: Harici komutları çalıştırmak için kullanılır.\n",
        "- DNS sunucusunun yapılandırılması: Activeloop Deep Lake kütüphanesinin çalışması için gereklidir.\n",
        "- Kimlik doğrulama: OpenAI ve Activeloop API'larına erişmek için gereklidir.\n",
        "\n",
        "## Bu İşlemlerin Kullanılabileceği Projeler\n",
        "- Makine öğrenimi projeleri\n",
        "- Derin öğrenme projeleri\n",
        "- Doğal dil işleme projeleri\n",
        "- Veri bilimi projeleri\n",
        "\n",
        "## Kaynaklar\n",
        "- [Google Colab](https://colab.research.google.com/)\n",
        "- [OpenAI](https://openai.com/)\n",
        "- [Activeloop](https://www.activeloop.ai/)\n",
        "- [Deep Lake](https://docs.activeloop.ai/)"
      ],
      "metadata": {
        "id": "BTa6B8tyDSjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Data Collection and Preparation"
      ],
      "metadata": {
        "id": "-v88ZbU8JJWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Veri Toplama ve Hazırlama (Data Collection and Preparation)\n",
        "Veri toplama ve hazırlama, bir makine öğrenimi (Machine Learning) veya doğal dil işleme (Natural Language Processing) projesinin ilk aşamasıdır. Bu aşamada, projenin gerektirdiği veriler toplanır, işlenir ve uygun bir formatta saklanır.\n",
        "\n",
        "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781836200918/files/Images/B31169_02_07.png)\n",
        "\n",
        "### Kullanılan Teknikler\n",
        "Bu projede, veri toplama ve hazırlama aşamasında aşağıdaki teknikler kullanılmıştır:\n",
        "* **HTTP İstekleri (HTTP Requests)**: `requests` kütüphanesi kullanılarak Wikipedia makalelerine HTTP istekleri gönderilir ve içerik alınır.\n",
        "* **HTML Ayrıştırma (HTML Parsing)**: `BeautifulSoup` kütüphanesi kullanılarak alınan HTML içerik ayrıştırılır ve gerekli bilgiler çıkarılır.\n",
        "* **Düzenli İfadeler (Regular Expressions)**: `re` kütüphanesi kullanılarak metin içerisindeki sayısal referanslar kaldırılır.\n",
        "\n",
        "### Kod Açıklamaları\n",
        "#### Veri Toplama\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Wikipedia makalelerinin URL'leri\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/Space_exploration\",\n",
        "    \"https://en.wikipedia.org/wiki/Apollo_program\",\n",
        "    \"https://en.wikipedia.org/wiki/Hubble_Space_Telescope\",\n",
        "    \"https://en.wikipedia.org/wiki/Mars_rover\",\n",
        "    \"https://en.wikipedia.org/wiki/International_Space_Station\",\n",
        "    \"https://en.wikipedia.org/wiki/SpaceX\",\n",
        "    \"https://en.wikipedia.org/wiki/Juno_(spacecraft)\",\n",
        "    \"https://en.wikipedia.org/wiki/Voyager_program\",\n",
        "    \"https://en.wikipedia.org/wiki/Galileo_(spacecraft)\",\n",
        "    \"https://en.wikipedia.org/wiki/Kepler_Space_Telescope\"\n",
        "]\n",
        "```\n",
        "Bu kodda, Wikipedia makalelerinin URL'leri bir liste içerisinde tanımlanmıştır.\n",
        "\n",
        "#### Veri Hazırlama\n",
        "```python\n",
        "def clean_text(content):\n",
        "    # Sayısal referansları kaldır\n",
        "    content = re.sub(r'\\[\\d+\\]', '', content)\n",
        "    return content\n",
        "\n",
        "def fetch_and_clean(url):\n",
        "    # URL'den içerik al\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # Ana içerik kısmını bul\n",
        "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
        "    # Bibliyografya kısmını kaldır\n",
        "    for section_title in ['References', 'Bibliography', 'External links', 'See also']:\n",
        "        section = content.find('span', id=section_title)\n",
        "        if section:\n",
        "            # Bu kısımdan sonraki tüm içerikleri kaldır\n",
        "            for sib in section.parent.find_next_siblings():\n",
        "                sib.decompose()\n",
        "            section.parent.decompose()\n",
        "    # Metni çıkar ve temizle\n",
        "    text = content.get_text(separator=' ', strip=True)\n",
        "    text = clean_text(text)\n",
        "    return text\n",
        "```\n",
        "Bu kodda, `clean_text` fonksiyonu sayısal referansları kaldırır, `fetch_and_clean` fonksiyonu ise URL'den içerik alır, ana içerik kısmını bulur, bibliyografya kısmını kaldırır ve metni çıkarır.\n",
        "\n",
        "#### Veri Yazma\n",
        "```python\n",
        "with open('llm.txt', 'w', encoding='utf-8') as file:\n",
        "    for url in urls:\n",
        "        clean_article_text = fetch_and_clean(url)\n",
        "        file.write(clean_article_text + '\\n')\n",
        "print(\"İçerik llm.txt dosyasına yazıldı\")\n",
        "```\n",
        "Bu kodda, temizlenmiş içerikler `llm.txt` dosyasına yazılır.\n",
        "\n",
        "### Neden Böyle Yapıldı?\n",
        "Veri toplama ve hazırlama aşaması, makine öğrenimi ve doğal dil işleme projelerinde önemlidir çünkü:\n",
        "* **Veri kalitesi**: Veri kalitesi, modelin performansı üzerinde büyük bir etkiye sahiptir.\n",
        "* **Veri formatı**: Veri formatı, modelin gerektirdiği formatta olmalıdır.\n",
        "\n",
        "Bu projede, Wikipedia makaleleri toplanmış ve temizlenmiştir. Bu, makine öğrenimi modellerinin eğitilmesi için uygun bir veri kümesi oluşturur.\n",
        "\n",
        "### Kullanılabilecek Projeler\n",
        "Bu teknikler, aşağıdaki projelerde kullanılabilir:\n",
        "* **Metin sınıflandırma**: Metinleri sınıflandırmak için makine öğrenimi modelleri eğitilebilir.\n",
        "* **Dil modelleme**: Dil modelleri eğitilebilir ve metin oluşturma, çeviri gibi görevlerde kullanılabilir.\n",
        "\n",
        "### Kaynaklar\n",
        "* [BeautifulSoup documentation](https://beautiful-soup-4.readthedocs.io/en/latest/)\n",
        "* [Requests documentation](https://requests.readthedocs.io/en/latest/)\n",
        "* [Regular Expressions documentation](https://docs.python.org/3/library/re.html)\n",
        "* [Step Step Data Collection](https://www.couchbase.com/blog/guide-to-data-prep-for-rag/)\n",
        "* [Prepare Data For RAG - İBM](https://developer.ibm.com/tutorials/dpk-rag-llms/)\n",
        "* [How to make data ready for your RAG application with Qdrant and FastEmbed](https://www.youtube.com/watch?v=2CESrNJ3NRI)\n",
        "* [Optimising Data for Advanced AI Responses](https://www.youtube.com/watch?v=pIGRwMjhMaQ)\n",
        "* [Best Tool For Getting Your Data Ready For RAG](https://www.youtube.com/watch?v=gvY4FgMjZUE)\n",
        "* [Python RAG Tutorial (with Local LLMs): AI For Your PDFs](https://www.youtube.com/watch?v=2TJxpyO3ei4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y_Q2AWC7JQlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Data embedding and storage"
      ],
      "metadata": {
        "id": "ipNfbRiUOe92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Veri Gömmek ve Depolamak (Data Embedding and Storage)\n",
        "\n",
        "Bu bölümde, veri gömmek ve depolamak için kullanılan teknikler ve araçlar ele alınacaktır. Veri gömmek, metin verilerini vektörlere dönüştürme işlemidir. Bu işlem, metinlerin anlamını ve içeriğini daha iyi temsil etmelerini sağlar.\n",
        "\n",
        "### Veri Hazırlama (Data Preparation)\n",
        "\n",
        "İlk olarak, veri hazırlama aşamasında, Team #1 tarafından hazırlanan veriler alınır. Bu veriler, daha sonra işlenmek üzere hazır hale getirilir.\n",
        "\n",
        "```python\n",
        "from grequests import download\n",
        "source_text = \"llm.txt\"\n",
        "directory = \"Chapter02\"\n",
        "filename = \"llm.txt\"\n",
        "download(directory, filename)\n",
        "```\n",
        "\n",
        "### Veri Okuma ve Doğrulama (Data Reading and Verification)\n",
        "\n",
        "İndirilen veri, ilk 20 satır okunarak doğrulanır.\n",
        "\n",
        "```python\n",
        "# Open the file and read the first 20 lines\n",
        "with open('llm.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "    # Print the first 20 lines\n",
        "    for line in lines[:20]:\n",
        "        print(line.strip())\n",
        "```\n",
        "\n",
        "### Veri Parçalama (Chunking)\n",
        "\n",
        "Veri, daha sonra belirli bir boyutta parçalara ayrılır. Bu işlem, veri işleme ve gömme işlemlerini optimize etmek için yapılır.\n",
        "\n",
        "```python\n",
        "with open(source_text, 'r') as f:\n",
        "    text = f.read()\n",
        "CHUNK_SIZE = 1000\n",
        "chunked_text = [text[i:i+CHUNK_SIZE] for i in range(0,len(text), CHUNK_SIZE)]\n",
        "```\n",
        "\n",
        "### Vektör Deposu Oluşturma (Creating a Vector Store)\n",
        "\n",
        "Vektör deposu, veri gömmek için kullanılır. Activeloop vektör deposu yolu tanımlanır ve depo oluşturulur.\n",
        "\n",
        "```python\n",
        "vector_store_path = \"hub://denis76/space_exploration_v1\"\n",
        "```\n",
        "\n",
        "Vektör deposu oluşturma veya yükleme işlemi yapılır.\n",
        "\n",
        "```python\n",
        "from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\n",
        "import deeplake.util\n",
        "try:\n",
        "    # Attempt to load the vector store\n",
        "    vector_store = VectorStore(path=vector_store_path)\n",
        "    print(\"Vector store exists\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Vector store does not exist. You can create it.\")\n",
        "    # Code to create the vector store goes here\n",
        "    create_vector_store=True\n",
        "```\n",
        "\n",
        "### Gömme Fonksiyonu (Embedding Function)\n",
        "\n",
        "Gömme fonksiyonu, veri parçalarını vektörlere dönüştürmek için kullanılır. Bu örnekte, \"text-embedding-3-small\" modeli kullanılır.\n",
        "\n",
        "```python\n",
        "def embedding_function(texts, model=\"text-embedding-3-small\"):\n",
        "   if isinstance(texts, str):\n",
        "       texts = [texts]\n",
        "   texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
        "   return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data]\n",
        "```\n",
        "\n",
        "### Veri Ekleme (Adding Data to the Vector Store)\n",
        "\n",
        "Vektör deposuna veri eklemek için aşağıdaki kod kullanılır.\n",
        "\n",
        "```python\n",
        "add_to_vector_store=True\n",
        "if add_to_vector_store == True:\n",
        "    with open(source_text, 'r') as f:\n",
        "        text = f.read()\n",
        "        CHUNK_SIZE = 1000\n",
        "        chunked_text = [text[i:i+1000] for i in range(0, len(text), CHUNK_SIZE)]\n",
        "vector_store.add(text = chunked_text,\n",
        "              embedding_function = embedding_function,\n",
        "              embedding_data = chunked_text,\n",
        "              metadata = [{\"source\": source_text}]*len(chunked_text))\n",
        "```\n",
        "\n",
        "### Vektör Deposu Bilgileri (Vector Store Information)\n",
        "\n",
        "Vektör deposu bilgileri, Activeloop'un API referansı kullanılarak elde edilebilir.\n",
        "\n",
        "```python\n",
        "ds = deeplake.load(vector_store_path)\n",
        "```\n",
        "\n",
        "Vektör deposu boyutu, aşağıdaki kod kullanılarak hesaplanabilir.\n",
        "\n",
        "```python\n",
        "#Estimates the size in bytes of the dataset.\n",
        "ds_size=ds.size_approx()\n",
        "# Convert bytes to megabytes and limit to 5 decimal places\n",
        "ds_size_mb = ds_size / 1048576\n",
        "print(f\"Dataset size in megabytes: {ds_size_mb:.5f} MB\")\n",
        "# Convert bytes to gigabytes and limit to 5 decimal places\n",
        "ds_size_gb = ds_size / 1073741824\n",
        "print(f\"Dataset size in gigabytes: {ds_size_gb:.5f} GB\")\n",
        "```\n",
        "\n",
        "## Kullanılan Teknikler ve Araçlar\n",
        "\n",
        "*   Veri gömmek için \"text-embedding-3-small\" modeli kullanılır.\n",
        "*   Vektör deposu olarak Activeloop kullanılır.\n",
        "*   Veri parçalama işlemi, veri işleme ve gömme işlemlerini optimize etmek için yapılır.\n",
        "\n",
        "## Neden Bu Teknikler Kullanılır?\n",
        "\n",
        "*   Veri gömmek, metin verilerini vektörlere dönüştürerek anlamını ve içeriğini daha iyi temsil etmelerini sağlar.\n",
        "*   Vektör deposu, veri gömmek için kullanılır ve veri sorgulama işlemlerini hızlandırır.\n",
        "\n",
        "## Bu Tekniklerin Kullanılabileceği Projeler\n",
        "\n",
        "*   Doğal dil işleme (NLP) projeleri\n",
        "*   Metin sınıflandırma projeleri\n",
        "*   Öneri sistemleri projeleri\n",
        "\n",
        "## Kaynaklar\n",
        "\n",
        "*   [Activeloop](https://www.activeloop.ai/)\n",
        "*   [OpenAI Embedding Models](https://platform.openai.com/docs/models/embeddings)\n",
        "*   [Deep Lake API Reference](https://docs.deeplake.ai/en/latest/)\n",
        "* [Deep Lake API Reference](https://docs.deeplake.ai/en/latest/)\n",
        "* [Vector Search RAG Tutorial](https://www.youtube.com/watch?v=JEBDfGqrAUA)\n",
        "* [Embeddings, Vector Databases, Similarity Search for RAG Systems Explained](https://www.youtube.com/watch?v=whu7SHx5cko)\n",
        "* [Mastering RAG: A Deep Dive into Embeddings](https://medium.com/@shravankoninti/mastering-rag-a-deep-dive-into-embeddings-b78782aa1259)\n",
        "* [How to Store Embeddings in Vector Search and Implement RAG](https://thenewstack.io/how-to-store-embeddings-in-vector-search-and-implement-rag/)\n",
        "* [Vector Embeddings in RAG Applications](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Vector-Embeddings-in-RAG-Applications--Vmlldzo3OTk1NDA5)\n",
        "* [The Limitations of Text Embeddings in RAG Applications](https://neo4j.com/blog/developer/rag-text-embeddings-limitations/)\n",
        "* [Understanding Embeddings in RAG and How to use them - Llama-Index](https://www.youtube.com/watch?v=v6g8eo86T8A)\n",
        "* [How to automate embedding creation and sync for RAG](https://www.youtube.com/watch?v=j2B5fx1p1Ps)\n",
        "* [How to Use Multimodal RAG to Extract Text, Images, & Tables](https://www.youtube.com/watch?v=jDFpEnJeSVg)\n",
        "* [Advanced RAG: Chunking, Embeddings, and Vector Databases 🚀 | LLMOps](https://www.youtube.com/watch?v=tTW3dOfyCpE)\n",
        "* [Build high-performance RAG using just PostgreSQL (Full Tutorial)](https://www.youtube.com/watch?v=hAdEuDBN57g)\n",
        "* [Building Real-Time RAG Pipeline With Mongodb and Pinecone Part-1](https://www.youtube.com/watch?v=lJl_bIXO7_Y)\n",
        "* [Practical RAG - Choosing the Right Embedding Model, Chunking Strategy, and More](https://www.youtube.com/watch?v=j1XRLh7yzzY)\n",
        "* [Advanced RAG Performance: Smart Data Handling Strategies](https://www.youtube.com/watch?v=oSZCIwmp65I)\n",
        "* [RAG with OpenAI & Pinecone Vector Database](https://www.youtube.com/watch?v=IuXVTJm-iF8)\n",
        "* [GenAI 101: Getting Started with a Vector Database](https://www.youtube.com/watch?v=1N-938QsddI)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ai5b3gxbOmuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Augmented input generation"
      ],
      "metadata": {
        "id": "FKz3-D53VWAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmented Input Generation (Geliştirilmiş Giriş Üretimi)\n",
        "Augmented generation, bir makine öğrenimi işlem hattının (pipeline) üçüncü bileşenidir. Kullanıcı girdisini geliştirmek için alınan verileri kullanır. Bu bileşen, kullanıcı girdisini işler, vektör deposunu (vector store) sorgular, girdiyi geliştirir ve gpt-4-turbo'yu çağırır.\n",
        "\n",
        "### İşlem Adımları\n",
        "1. **Vektör Deposu Seçimi**: İlk olarak, vektör deposu yolu belirlenir.\n",
        "   ```python\n",
        "vector_store_path = \"hub://denis76/space_exploration_v1\"\n",
        "```\n",
        "2. **Veri Yükleme**: Deeplake kütüphanesi kullanılarak vektör deposu yüklenir.\n",
        "   ```python\n",
        "from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\n",
        "import deeplake.util\n",
        "ds = deeplake.load(vector_store_path)\n",
        "vector_store = VectorStore(path=vector_store_path)\n",
        "```\n",
        "3. **Kullanıcı Girdisi ve Sorgu Alma**: Kullanıcı girdisi alınır ve bu girdi, daha önce yüklenen vektör deposunda sorgulama yapmak için kullanılır.\n",
        "   ```python\n",
        "def get_user_prompt():\n",
        "    return input(\"Enter your search query: \")\n",
        "user_prompt = \"Tell me about space exploration on the Moon and Mars.\"\n",
        "```\n",
        "\n",
        "4. **Sorgu İşlemi**: Kullanıcı girdisi, vektör deposunda sorgulanır ve en ilgili sonuçlar alınır.\n",
        "   ```python\n",
        "search_results = vector_store.search(embedding_data=user_prompt, embedding_function=embedding_function)\n",
        "```\n",
        "\n",
        "5. **Gelişmiş Girdi Oluşturma**: Alınan sonuçlar içinden en üstteki sonuç, kullanıcı girdisine eklenir.\n",
        "   ```python\n",
        "top_text = search_results['text'][0].strip()\n",
        "augmented_input = user_prompt + \" \" + top_text\n",
        "```\n",
        "\n",
        "6. **GPT-4 ile İçerik Üretimi**: Gelişmiş girdi, GPT-4 modeline verilerek içerik üretilir.\n",
        "   ```python\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "gpt_model = \"gpt-4o\"\n",
        "def call_gpt4_with_full_text(itext):\n",
        "    # GPT-4'e çağrı yapar ve yanıtı döndürür.\n",
        "    response = client.chat.completions.create(\n",
        "        model=gpt_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a space exploration expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"You can read the input and answer in detail.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.1\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "gpt4_response = call_gpt4_with_full_text(augmented_input)\n",
        "```\n",
        "\n",
        "7. **Yanıtın Biçimlendirilmesi**: Üretilen içerik, Markdown biçimlendirmesi kullanılarak daha okunabilir hale getirilir.\n",
        "   ```python\n",
        "import textwrap\n",
        "import re\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import markdown\n",
        "def print_formatted_response(response):\n",
        "    # Markdown desenlerini arar ve uygun şekilde biçimlendirir.\n",
        "    if any(re.search(pattern, response, re.MULTILINE) for pattern in markdown_patterns):\n",
        "        html_output = markdown.markdown(response)\n",
        "        display(HTML(html_output))\n",
        "    else:\n",
        "        wrapper = textwrap.TextWrapper(width=80)\n",
        "        wrapped_text = wrapper.fill(text=response)\n",
        "        print(wrapped_text)\n",
        "print_formatted_response(gpt4_response)\n",
        "```\n",
        "\n",
        "### Kullanılan Teknikler ve Kütüphaneler\n",
        "- **Deeplake**: Vektör deposu olarak kullanılan bir kütüphane. Veri saklama ve sorgulama işlemlerini kolaylaştırır.\n",
        "- **OpenAI**: GPT-4 gibi gelişmiş dil modellerine erişim sağlar.\n",
        "- **Markdown**: Metin biçimlendirme dili. İçerikleri daha okunabilir hale getirmek için kullanılır.\n",
        "\n",
        "### Neden Bu Teknikler Kullanıldı?\n",
        "- **Gelişmiş Girdi Oluşturma**: Kullanıcı girdisini, ilgili bilgilerle zenginleştirerek daha doğru ve detaylı içerik üretilmesini sağlar.\n",
        "- **Vektör Deposu**: İlgili içeriklerin hızlı ve etkin bir şekilde sorgulanmasını sağlar.\n",
        "\n",
        "### Kullanılabilecek Projeler\n",
        "- **Chatbot Sistemleri**: Kullanıcılara daha doğru ve alakalı yanıtlar vermek için kullanılabilir.\n",
        "- **İçerik Üretimi**: Otomatik içerik üretimi için kullanılabilir. Örneğin, belirli bir konuda makale veya rapor yazımı.\n",
        "\n",
        "### Kaynaklar\n",
        "- [Deeplake Resmi Dokümantasyonu](https://docs.deeplake.ai/en/latest/)\n",
        "- [OpenAI API Dokümantasyonu](https://platform.openai.com/docs/api-reference)\n",
        "- [Markdown Rehberi](https://www.markdownguide.org/)\n",
        "\n",
        "\n",
        "### YouTube Kaynakları\n",
        "* [GenAI 101: Getting Started with a Vector Database](https://www.youtube.com/watch?v=1N-938QsddI)\n",
        "* [Vector Search RAG Tutorial](https://www.youtube.com/watch?v=JEBDfGqrAUA)\n",
        "* [How to Use Multimodal RAG to Extract Text, Images, & Tables](https://www.youtube.com/watch?v=jDFpEnJeSVg)\n",
        "\n",
        "### İngilizce Kaynaklar\n",
        "* [Mastering RAG: A Deep Dive into Embeddings](https://medium.com/@shravankoninti/mastering-rag-a-deep-dive-into-embeddings-b78782aa1259)\n",
        "* [How to Store Embeddings in Vector Search and Implement RAG](https://thenewstack.io/how-to-store-embeddings-in-vector-search-and-implement-rag/)\n",
        "* [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (ArXiv)](https://arxiv.org/abs/2005.11401)\n",
        "\n"
      ],
      "metadata": {
        "id": "Im07eKk6Vf5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Similarity - Metrics"
      ],
      "metadata": {
        "id": "uYj33qdmXosx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity ile Çıktı Değerlendirme\n",
        "Bu bölümde, kullanıcı girdisi ve üretken yapay zeka modelinin çıktısı arasındaki benzerliği ölçmek için cosine similarity (kosinüs benzerliği) uygulayacağız. Ayrıca, genişletilmiş kullanıcı girdisi ile üretken yapay zeka modelinin çıktısı arasındaki benzerliği de ölçülecektir.\n",
        "\n",
        "### Kosinüs Benzerliği Fonksiyonu Tanımlama\n",
        "İlk olarak, kosinüs benzerliği fonksiyonunu tanımlayalım:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "```\n",
        "\n",
        "Bu fonksiyon, iki metin arasındaki kosinüs benzerliğini hesaplar. TF-IDF (Term Frequency-Inverse Document Frequency) vektörleştiricisi kullanılarak metinler vektör uzayına dönüştürülür ve ardından kosinüs benzerliği hesaplanır.\n",
        "\n",
        "### Kullanıcı Promptı ve GPT-4 Yanıtı Arasındaki Benzerlik\n",
        "Kullanıcı promptı ve GPT-4 yanıtı arasındaki benzerliği hesaplayalım:\n",
        "\n",
        "```python\n",
        "similarity_score = calculate_cosine_similarity(user_prompt, gpt4_response)\n",
        "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")\n",
        "```\n",
        "\n",
        "Çıktı:\n",
        "```\n",
        "Cosine Similarity Score: 0.396\n",
        "```\n",
        "\n",
        "Bu skor düşük görünmektedir, ancak çıktı insan için kabul edilebilir görünmektedir.\n",
        "\n",
        "### Genişletilmiş Kullanıcı Girdisi ve GPT-4 Yanıtı Arasındaki Benzerlik\n",
        "Genişletilmiş kullanıcı girdisi ve GPT-4 yanıtı arasındaki benzerliği hesaplayalım:\n",
        "\n",
        "```python\n",
        "similarity_score = calculate_cosine_similarity(augmented_input, gpt4_response)\n",
        "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")\n",
        "```\n",
        "\n",
        "Çıktı:\n",
        "```\n",
        "Cosine Similarity Score: 0.857\n",
        "```\n",
        "\n",
        "Bu skor daha iyi görünmektedir.\n",
        "\n",
        "### Kosinüs Benzerliği Sınırlamaları\n",
        "Kosinüs benzerliği, TF-IDF kullanırken tam kelime örtüşmesine bağlıdır ve anlamsal anlamlar, eş anlamlılar veya bağlamsal kullanım gibi önemli dil özelliklerini dikkate alır. Bu nedenle, kavramsal olarak benzer ancak kelime seçimi farklı olan metinler için daha düşük benzerlik skorları üretebilir.\n",
        "\n",
        "### Cümle Transformatörleri ile Benzerlik Hesaplama\n",
        "Cümle Transformatörleri, kelimeler ve cümleler arasındaki daha derin anlamsal ilişkileri yakalayan embeddings kullanır. Bu yaklaşım, metinler arasındaki bağlamsal ve kavramsal benzerliği tanımada daha etkilidir.\n",
        "\n",
        "İlk olarak, sentence-transformers kütüphanesini kuralım:\n",
        "\n",
        "```bash\n",
        "!pip install sentence-transformers\n",
        "```\n",
        "\n",
        "Ardından, MiniLM mimarisini kullanarak benzerlik hesaplayalım:\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def calculate_cosine_similarity_with_embeddings(text1, text2):\n",
        "    embeddings1 = model.encode(text1)\n",
        "    embeddings2 = model.encode(text2)\n",
        "    similarity = cosine_similarity([embeddings1], [embeddings2])\n",
        "    return similarity[0][0]\n",
        "```\n",
        "\n",
        "Bu fonksiyon, iki metin arasındaki kosinüs benzerliğini embeddings kullanarak hesaplar.\n",
        "\n",
        "### Genişletilmiş Kullanıcı Girdisi ve GPT-4 Yanıtı Arasındaki Benzerlik (Cümle Transformatörleri ile)\n",
        "Genişletilmiş kullanıcı girdisi ve GPT-4 yanıtı arasındaki benzerliği Cümle Transformatörleri ile hesaplayalım:\n",
        "\n",
        "```python\n",
        "similarity_score = calculate_cosine_similarity_with_embeddings(augmented_input, gpt4_response)\n",
        "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")\n",
        "```\n",
        "\n",
        "Çıktı:\n",
        "```\n",
        "Cosine Similarity Score: 0.739\n",
        "```\n",
        "\n",
        "Bu skor, Cümle Transformatörleri'nin metinler arasındaki anlamsal benzerliği daha etkili bir şekilde yakaladığını göstermektedir.\n",
        "\n",
        "## Sonuç\n",
        "Kosinüs benzerliği ve Cümle Transformatörleri, metinler arasındaki benzerliği ölçmede farklı yaklaşımlar sunar. Proje gereksinimlerine bağlı olarak, uygun metrik seçilmelidir.\n",
        "\n",
        "## İlgili Kaynaklar\n",
        "\n",
        "* [Sentence Transformers](https://www.sbert.net/)\n",
        "* [Hugging Face Model Hub](https://huggingface.co/models)\n",
        "* [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "\n",
        "## Kullanılabilecek Projeler\n",
        "Bu teknikler, doğal dil işleme (NLP) projelerinde, özellikle de metin benzerliği ve anlamsal arama uygulamalarında kullanılabilir. Örneğin:\n",
        "\n",
        "* Metin sınıflandırma\n",
        "* Anlamsal arama\n",
        "* Otomatik özetleme\n",
        "* Makine çevirisi değerlendirme\n",
        "\n",
        "Bu tekniklerin kullanılabileceği diğer projeler için [NLP uygulamaları](https://www.nlpapplications.com/) sayfasını ziyaret edebilirsiniz."
      ],
      "metadata": {
        "id": "Q_uWLz1BXuXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "OpenAI Ada documentation for embeddings:\n",
        "https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
        "\n",
        "OpenAI GPT documentation for content generation:\n",
        " https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\n",
        "\n",
        "Activeloop API documentation:\n",
        "https://docs.deeplake.ai/en/latest/\n",
        "\n",
        "MiniLM model reference:\n",
        "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
      ],
      "metadata": {
        "id": "sNzIqbyiYWmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBILfsT9NMXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b779b4-f772-4098-e627-35f5c78fa92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Cosine Similarity ile Çıktı Değerlendirme\n",
            "Bu bölümde, kullanıcı girdisi ve üretken yapay zeka modelinin çıktısı arasındaki benzerliği ölçmek için cosine similarity (kosinüs benzerliği) uygulayacağız. Ayrıca, genişletilmiş kullanıcı girdisi ile üretken yapay zeka modelinin çıktısı arasındaki benzerliği de ölçülecektir.\n",
            "\n",
            "### Kosinüs Benzerliği Fonksiyonu Tanımlama\n",
            "İlk olarak, kosinüs benzerliği fonksiyonunu tanımlayalım:\n",
            "\n",
            "```python\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.metrics.pairwise import cosine_similarity\n",
            "\n",
            "def calculate_cosine_similarity(text1, text2):\n",
            "    vectorizer = TfidfVectorizer()\n",
            "    tfidf = vectorizer.fit_transform([text1, text2])\n",
            "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
            "    return similarity[0][0]\n",
            "```\n",
            "\n",
            "Bu fonksiyon, iki metin arasındaki kosinüs benzerliğini hesaplar. TF-IDF (Term Frequency-Inverse Document Frequency) vektörleştiricisi kullanılarak metinler vektör uzayına dönüştürülür ve ardından kosinüs benzerliği hesaplanır.\n",
            "\n",
            "### Kullanıcı Promptı ve GPT-4 Yanıtı Arasındaki Benzerlik\n",
            "Kullanıcı promptı ve GPT-4 yanıtı arasındaki benzerliği hesaplayalım:\n",
            "\n",
            "```python\n",
            "similarity_score = calculate_cosine_similarity(user_prompt, gpt4_response)\n",
            "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")\n",
            "```\n",
            "\n",
            "Çıktı:\n",
            "```\n",
            "Cosine Similarity Score: 0.396\n",
            "```\n",
            "\n",
            "Bu skor düşük görünmektedir, ancak çıktı insan için kabul edilebilir görünmektedir.\n",
            "\n",
            "### Genişletilmiş Kullanıcı Girdisi ve GPT-4 Yanıtı Arasındaki Benzerlik\n",
            "Genişletilmiş kullanıcı girdisi ve GPT-4 yanıtı arasındaki benzerliği hesaplayalım:\n",
            "\n",
            "```python\n",
            "similarity_score = calculate_cosine_similarity(augmented_input, gpt4_response)\n",
            "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")\n",
            "```\n",
            "\n",
            "Çıktı:\n",
            "```\n",
            "Cosine Similarity Score: 0.857\n",
            "```\n",
            "\n",
            "Bu skor daha iyi görünmektedir.\n",
            "\n",
            "### Kosinüs Benzerliği Sınırlamaları\n",
            "Kosinüs benzerliği, TF-IDF kullanırken tam kelime örtüşmesine bağlıdır ve anlamsal anlamlar, eş anlamlılar veya bağlamsal kullanım gibi önemli dil özelliklerini dikkate alır. Bu nedenle, kavramsal olarak benzer ancak kelime seçimi farklı olan metinler için daha düşük benzerlik skorları üretebilir.\n",
            "\n",
            "### Cümle Transformatörleri ile Benzerlik Hesaplama\n",
            "Cümle Transformatörleri, kelimeler ve cümleler arasındaki daha derin anlamsal ilişkileri yakalayan embeddings kullanır. Bu yaklaşım, metinler arasındaki bağlamsal ve kavramsal benzerliği tanımada daha etkilidir.\n",
            "\n",
            "İlk olarak, sentence-transformers kütüphanesini kuralım:\n",
            "\n",
            "```bash\n",
            "!pip install sentence-transformers\n",
            "```\n",
            "\n",
            "Ardından, MiniLM mimarisini kullanarak benzerlik hesaplayalım:\n",
            "\n",
            "```python\n",
            "from sentence_transformers import SentenceTransformer\n",
            "\n",
            "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
            "\n",
            "def calculate_cosine_similarity_with_embeddings(text1, text2):\n",
            "    embeddings1 = model.encode(text1)\n",
            "    embeddings2 = model.encode(text2)\n",
            "    similarity = cosine_similarity([embeddings1], [embeddings2])\n",
            "    return similarity[0][0]\n",
            "```\n",
            "\n",
            "Bu fonksiyon, iki metin arasındaki kosinüs benzerliğini embeddings kullanarak hesaplar.\n",
            "\n",
            "### Genişletilmiş Kullanıcı Girdisi ve GPT-4 Yanıtı Arasındaki Benzerlik (Cümle Transformatörleri ile)\n",
            "Genişletilmiş kullanıcı girdisi ve GPT-4 yanıtı arasındaki benzerliği Cümle Transformatörleri ile hesaplayalım:\n",
            "\n",
            "```python\n",
            "similarity_score = calculate_cosine_similarity_with_embeddings(augmented_input, gpt4_response)\n",
            "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")\n",
            "```\n",
            "\n",
            "Çıktı:\n",
            "```\n",
            "Cosine Similarity Score: 0.739\n",
            "```\n",
            "\n",
            "Bu skor, Cümle Transformatörleri'nin metinler arasındaki anlamsal benzerliği daha etkili bir şekilde yakaladığını göstermektedir.\n",
            "\n",
            "## Sonuç\n",
            "Kosinüs benzerliği ve Cümle Transformatörleri, metinler arasındaki benzerliği ölçmede farklı yaklaşımlar sunar. Proje gereksinimlerine bağlı olarak, uygun metrik seçilmelidir.\n",
            "\n",
            "## İlgili Kaynaklar\n",
            "\n",
            "* [Sentence Transformers](https://www.sbert.net/)\n",
            "* [Hugging Face Model Hub](https://huggingface.co/models)\n",
            "* [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
            "\n",
            "## Kullanılabilecek Projeler\n",
            "Bu teknikler, doğal dil işleme (NLP) projelerinde, özellikle de metin benzerliği ve anlamsal arama uygulamalarında kullanılabilir. Örneğin:\n",
            "\n",
            "* Metin sınıflandırma\n",
            "* Anlamsal arama\n",
            "* Otomatik özetleme\n",
            "* Makine çevirisi değerlendirme\n",
            "\n",
            "Bu tekniklerin kullanılabileceği diğer projeler için [NLP uygulamaları](https://www.nlpapplications.com/) sayfasını ziyaret edebilirsiniz.\n"
          ]
        }
      ],
      "source": [
        "from together import Together\n",
        "\n",
        "def ask_together(content):\n",
        "\n",
        "    client = Together()\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
        "    messages=[{\"role\":\"user\",\"content\":content},{\"role\":\"user\",\"content\":\"\"}],\n",
        "    temperature=0.51,\n",
        "    top_p=0.91,\n",
        "    seed=198\n",
        ")\n",
        "    return print(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "soru = \"\"\"\n",
        "\n",
        "Aşağıda paragrafı türkçe olarak açıkla ama ingilizce teknik kavramlarıda yanına ekle , Konu ile ilgili aşağıdaki text e bağlı kalmadan ve önemli noktalar ekle , Aşağıda kullanılan teknikler dşında başka teknikler var ise onlarıda yaz,kodları açıkla , neden böyle yapıldığını yaz , ayrıca bu işlemin ne tür projelerde kullanılabileceğini ifade et , ayrıca konuyla ilgili internetteki kaynaklardan alacağın linkleride ekle , ayrıca markdown olarak ## ifadeleri ile yaz, Lütfen tüm kodları eksiksiz yaz ve resim linklerini verdim onlarıda ilgili yerde göster.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ask_together(soru)"
      ]
    }
  ]
}